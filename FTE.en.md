# Feynman Technique Evaluation
> [!WARNING]
> This is an AI-translated version. The original and definitive version is in Chinese.
> 
> **[Back to the original Chinese version (返回中文原文)](./README.md)**
## 1. Purpose

This framework aims to evaluate whether a Large Language Model (LLM) has achieved a genuine "understanding" of a given topic, drawing inspiration from the core principles of the Feynman Technique: simplifying complexity and learning through teaching. It provides a comprehensive assessment of the LLM's capabilities in abstraction, compression, and reconstruction of knowledge.

## 2. Design Principles

The Feynman Technique consists of two core stages: **Deep Understanding** and **Teaching Verification**.

### 2.1. Deep Understanding (Knowledge Compression)

True understanding of a concept is demonstrated by the ability to re-encode and express it using one's existing knowledge framework, resulting in a highly personalized and information-dense representation. This expression is an equivalent yet simplified description of the original knowledge. The essence of this process is **to explain the unknown with the known**. By filtering and compressing the information through a personal knowledge structure, a refined "information core" is created.

### 2.2. Teaching Verification (Knowledge Reconstruction)

An effective test of this understanding is the ability to teach the concept to others. We can envision a compelling scenario: **teaching it to one's "pre-learning self."** Since the fundamental knowledge structure is identical, an effective "teaching material" (the compressed information core) should allow this "pre-learning self" to losslessly reconstruct the original knowledge from it. For an LLM, this "pre-learning self" can be easily simulated by initiating a new, clean session.

## 3. Evaluation Process

The evaluation process is defined as follows: **T(original) → LLM(Session 1) → T1 → LLM(Session 2) → T2**

*   **T(original)**: The original text.
*   **LLM(Session 1)**: The first LLM instance, responsible for understanding and compression.
*   **T1**: The "information core" generated by the LLM—maximally compressed and reconstructible.
*   **LLM(Session 2)**: A second LLM instance (or a clean session state), responsible for teaching and reconstruction.
*   **T2**: The text reconstructed by the LLM based solely on T1.

### 3.1. Prompt 1: Understanding Knowledge (T(original) → T1)

```
Your task is to distill the following [Original Text] into its most fundamental, irreducible [Information].
The goal for this [Information] is:
1.  **Extreme Compression**: Use the format and language you deem most effective and information-dense.
2.  **Reconstructibility**: Another equally capable model should be able to fully restore the original text's complete meaning, complexity, and style, based solely on this [Information].

It is unacceptable to provide any explanations or additional comments. Directly output the [Information] you generate.

[Original Text]
...
```

### 3.2. Prompt 2: Teaching Verification (T1 → T2)

```
Your task is to reconstruct the original text based solely on the [Information] provided below.
Your goal is:
1.  Your output must be based on all the content, relationships, and structures provided in the [Information].
2.  Fully utilize the content within the [Information].

[Information]
...
```

## 4. Evaluation Criteria

### 4.1 T1 Evaluation Criteria
*   **Information Completeness**: Assesses whether T1 includes **all key semantic units** required to restore T(original) (e.g., facts, opinions, logical relations, causal chains, key stylistic directives).
*   **Information Correctness**: Assesses whether the information in T1 is entirely consistent with T(original).

T1 Score = Weighted_Average(completeness_score, correctness_score, efficiency_score). For example: 0.5 * completeness + 0.4 * correctness + 0.1 * efficiency. Weights can be adjusted based on evaluation priorities.

### 4.2 T2 Evaluation Criteria

*   **Style Consistency**: Assesses whether the **tone, writing style, word choice, and sentence complexity** of T2 are highly consistent with T(original).
*   **Information Utilization - No Hallucination**: Assesses whether T2 introduces any key information or concepts not present in T1. A higher score indicates fewer hallucinations.
*   **Information Utilization - No Omission**: Assesses whether T2 fully utilizes all key elements and structures from T1. A higher score indicates more complete utilization.

T2 Score = Weighted_Average(semantic_fidelity_score, style_score, hallucination_check_score, coverage_check_score). For example: 0.3 * semantic_fidelity + 0.4 * style + 0.15 * hallucination_check + 0.15 * coverage_check. Weights can be adjusted based on evaluation priorities.

## Feynman Score

Feynman Score = (T1_Score * W1) + (T2_Score * W2). W1 is provisionally set to 0.6, and W2 to 0.4.

## 5. Test Results and Analysis

### Test Case Descriptions

1.  **Aphasia Meme**: Sourced from a popular Chinese internet meme, featuring a verbal dispute between an elderly woman and a security guard, likely over vehicle access to a residential community. The woman is speculated to have aphasia; though her language is chaotic, it appears to retain an underlying logic.
2.  **"RuoZhiBa" Q&A**: Sourced from a Q&A on "RuoZhiBa" (a satirical Chinese online community known for absurd questions and logical-fallacy-based humor).
3.  **sqrt(22)**: The value of √22 calculated to 999 decimal places.
4.  **Userscript**: A script for copying an address from a button on the Amap (Gaode Maps) website.

### Feynman Technique Evaluation - Overall Score Summary

This table summarizes the Feynman evaluation scores for Gemini 2.5 Pro and DeepSeek-R1-0528 across four different text types. The evaluation was performed by **Gemini 2.5 Pro**, covering **T1 (Understanding & Abstraction)**, **T2 (Teaching & Reconstruction)**, and the final **Feynman Score**.

| Case | Text Type | Model | T1 Score <br/>(Understanding & Abstraction) | T2 Score <br/>(Teaching & Reconstruction) | **Feynman Score (F-Score)** | System Diagnosis Summary |
| :--- | :--- | :--- | :---: | :---: | :---: | :--- |
| **1. Aphasia** | Chaotic, Non-linear | **Gemini 2.5 Pro** | **9.7 / 10** | 6.9 / 10 | **8.58 / 10** | **Strong understanding, but over-organizes during reconstruction**, destroying the original's chaotic style. |
| (Word Salad) | (High-entropy, Illogical) | **DeepSeek-R1-0528** | 9.5 / 10 | **7.8 / 10** | **8.82 / 10** | **Strong encoding ability**; T2 mechanically restored T1, coincidentally preserving the original's 'incoherence'. |
| | | | | | | |
| **2. Q&A** | Clear, Logical | **Gemini 2.5 Pro** | **9.8 / 10** | **9.2 / 10** | **9.56 / 10** | **Masterful logical analysis**; perfectly abstracted the argument structure and achieved high-quality reconstruction. |
| (RuoZhiBa) | (Ordered, Rational) | **DeepSeek-R1-0528** | 9.0 / 10 | 5.0 / 10 | 7.40 / 10 | **Over-compression and lazy reconstruction**; T1 lost context, and T2 was merely a copy of T1. |
| | | | | | | |
| **3. sqrt(22)** | Pure Symbols, No Semantics | **Gemini 2.5 Pro** | 1.0 / 10 | 5.5 / 10 | 2.80 / 10 | **Catastrophic failure**; forced pattern-seeking led to incorrect identification and complete hallucination. |
| (Data String) | (High-entropy, Purely Symbolic) | **DeepSeek-R1-0528** | **9.8 / 10** | **10.0 / 10**| **9.88 / 10** | **Excellent data processing mindset**; correctly employed an encoding/decoding strategy for lossless fidelity. |
| | | | | | | |
| **4. Code** | Structured, Logical | **Gemini 2.5 Pro** | **9.8 / 10** | **9.6 / 10** | **9.72 / 10** | **Architect-level understanding**; capable of abstracting the code's design intent and perfectly reconstructing it with comments. |
| (Userscript) | (Highly Structured) | **DeepSeek-R1-0528** | 8.6 / 10 | 6.2 / 10 | 7.64 / 10 | **Summarizer-level understanding**; T1 was a lossy summary, leading to an incomplete and stylistically barren T2. |

### My Analysis

1.  For Case 1, I believe Gemini's performance was superior overall. DeepSeek-R1's reconstructed text (T2) was actually weak, losing the original logic and entities. It was more of a paraphrase that coincidentally scored high because the source material was inherently chaotic.
2.  For Case 2, the model's analysis is quite accurate, so I have no further comments.
3.  For Case 3, both models actually failed completely. DeepSeek-R1 attempted to use base64, but both the encoding and decoding processes were incorrect. This suggests that LLMs are fundamentally incapable of handling this type of raw data integrity task.
4.  For Case 4, the model's analysis is also very accurate. However, I conducted an interesting follow-up experiment: I swapped the T1 outputs between the models. DeepSeek-R1 still failed to generate working code from Gemini's T1. Surprisingly, Gemini was able to use DeepSeek-R1's T1 to generate a T2 that was fully functional. This experiment strengthens my conviction that DeepSeek-R1 lacks the requisite knowledge in this domain.

## Citation

```bibtex
@misc{lanker2025feynman,
  author       = {lanker},
  title        = {{Feynman Technique Evaluation Framework for LLMs}},
  year         = {2025},
  publisher    = {GitHub},
  howpublished = {\url{https://github.com/thelanker/Feynman-Technique-Evaluation}}
}
```